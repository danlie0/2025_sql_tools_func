# Project Code Export
Generated on: Wed Aug 27 10:59:43 BST 2025
Folders: ..
Extensions: * dart js json yaml ts md txt sh py html css xml
Specific files: ../package.json ../host.json ../local.settings.json ../openapi.yaml ../README.md ../test_query.json

Project Structure
==================
.
  ├── ../
    ├── docs/
      ├── azure-sql-tools-prd.md
      ├── project-overview-donot-change.md
    ├── scripts/
      ├── code_aggregator.sh
      ├── deploy_func.sh
      ├── deploy_infra.sh
    ├── src/
      ├── functions/
        ├── sqlQuery.js
        ├── sqlSchema.js
      ├── lib/
        ├── db.js
        ├── schemaHelpers.js
        ├── sqlSafety.js
      ├── app.js
    ├── .env
    ├── .funcignore
    ├── README.md
    ├── host.json
    ├── local.settings.json
    ├── openapi.yaml
    ├── package.json
    ├── test_query.json

=== FILE: ../host.json ===
{
  "version": "2.0",
  "extensionBundle": { 
    "id": "Microsoft.Azure.Functions.ExtensionBundle", 
    "version": "[4.*, 5.0.0)" 
  }
}



=== FILE: ../.funcignore ===
# Files/folders excluded from Azure Functions deployment
local.settings.json
.env
.env.*
.vscode/
.git/
node_modules/
tests/
**/*.md
package-lock.json



=== FILE: ../docs/project-overview-donot-change.md ===
# SQL Tools for Copilot Studio - Project Overview

## What We're Building

A conversational agent that lets users ask natural language questions about our database data through Microsoft Teams or Copilot Studio 365 chat, and get responses directly in the same interface.

## The Goal

Enable users to ask questions like:
- "How many people signed up yesterday?"
- "What's our revenue this month?"
- "Show me the top 5 countries by users"
- "How many active users do we have?"

And get instant, accurate answers from our database - **without giving users direct database access**.

## How It Works

```
User asks question in Teams/Copilot Studio 365 chat
        ↓
Copilot Studio understands the question
        ↓
Connectory routes the request
        ↓
Azure Function App generates and executes SQL
        ↓
Safe, read-only results returned to Teams/Copilot Studio 365 chat
        ↓
User sees formatted answer in the same chat interface
```

## Why This Approach?

✅ **Security First**: No direct database access for users  
✅ **Self-Service**: Users get answers instantly without bothering developers  
✅ **Safe Queries**: Only read-only operations on approved views  
✅ **Natural Language**: No need to learn SQL  
✅ **Audit Trail**: All queries are logged for monitoring  

## What We're NOT Building

❌ Any write operations (INSERT, UPDATE, DELETE)  
❌ Access to raw tables with sensitive data  
❌ Complex reporting or dashboards  
❌ Data export functionality  

## Success Looks Like

Users can have conversations in Teams or Copilot Studio 365 chat like:
> **User**: "How's our user growth this week?"  
> **Agent**: "We had 1,247 new signups this week, which is 23% higher than last week (1,012 signups)."

> **User**: "Which countries are our biggest markets?"  
> **Agent**: "Top 5 countries by active users: 1) United States (45,231), 2) United Kingdom (12,847), 3) Canada (8,932), 4) Australia (6,721), 5) Germany (5,483)."

Simple, conversational, and **safe** - all within the familiar chat interfaces users already use.


=== FILE: ../docs/azure-sql-tools-prd.md ===
# PRD — Azure Functions "SQL Tools" (for Copilot Studio via Connectory)

**Owner:** Adam  
**Status:** Ready to build  
**Goal:** Let users ask questions in Teams/Copilot ("how many users signed up yesterday?"). Copilot Studio calls Connectory → Function App, which executes two tools:

- `sql_schema` — returns a whitelisted schema map (tables/views, columns, joins).
- `sql_query` — executes read‑only, parameterized T‑SQL and returns results (aggregates or small rowsets).

**Why:** Keep SQL access server‑side with strict guardrails while allowing Copilot Studio (via Connectory) to reason/generate queries.

## Scope

### In scope
- One Azure Function App (Node.js 20) with two HTTP endpoints (easy to test) that Connectory can call from Copilot Studio.
- Managed Identity (no secrets) to connect to Azure SQL.
- Hard safety rails: SELECT‑only, auto‑limit, parameterization.
- `.env` and `local.settings.json` for local dev.
- `scripts/deploy_infra.sh` (Azure CLI) to create RG/Storage/Function App + identity + app settings.
- `scripts/deploy_func.sh` to zip‑deploy the code.
- Readme notes + admin reminders (DB user, firewall, etc.).
- Query logging for audit trail
- Common query templates in schema response

### Out of scope
- Writes/DDL (INSERT/UPDATE/DELETE, DROP, ALTER, …).
- Private endpoints (can be added later).
- Vectorization or RAG.

## Key decisions

- **Language:** Node.js with mssql (no ODBC driver hassles).
- **Auth to SQL:** System‑assigned Managed Identity (MI) → AAD token → SQL `db_datareader` only.
- **Network:** For dev, allow "Allow Azure services…" on SQL Server. For prod, use private endpoints.
- **Tables exposed:** Use views (prefix with `vw`) as the allow‑list. The schema tool only emits those.
- **Time zone:** Treat relative dates (e.g., "yesterday") as UTC unless user specifies otherwise.

## Acceptance criteria

- GET `/api/sql-schema` returns JSON describing whitelisted views and columns.
- POST `/api/sql-query` accepts JSON and returns JSON with columns, rows, row_count, and sql_used.
- The code rejects non‑SELECT SQL, injects a `TOP(@row_limit)` if missing, and parameterizes values.
- Function App runs with MI; no connection strings or passwords anywhere.
- Deploy scripts work end‑to‑end from a clean subscription/resource group.
- Query execution is logged with metrics for monitoring

## Repo Layout

```
/sql-tools-func/
  package.json
  host.json
  .funcignore
  local.settings.json        # local only; do not commit secrets
  .env.example               # template for local env
  src/
    app.js                   # registers functions
    lib/
      db.js                  # MI auth + connect helpers
      sqlSafety.js           # validation, limit injection, param parsing
      schemaHelpers.js       # schema descriptions and query templates
    functions/
      sqlSchema.js           # GET/POST /api/sql-schema
      sqlQuery.js            # POST /api/sql-query
  scripts/
    deploy_infra.sh
    deploy_func.sh
  README.md
```

## Environment Variables

Create `.env` for local dev (Cursor should also produce `local.settings.json` with same values):

```bash
# .env (copy to local.settings.json Values)
SQL_SERVER=<your-sql-server>.database.windows.net
SQL_DATABASE=<your-db>
SQL_CONNECTION_STRING=  # Optional: for local dev only
SCHEMA_WHITELIST=vw%    # only expose views matching this
ROW_LIMIT_DEFAULT=200
TZ=UTC
NODE_ENV=development     # Set to 'production' in Azure
```

## Pre-deployment Checklist

☑️ Turn ON System‑Assigned MI on the Function App.  
☑️ In Azure SQL (database), create an AAD user for that MI and grant `db_datareader`:

```sql
CREATE USER [<func-app-mi-name>] FROM EXTERNAL PROVIDER;
ALTER ROLE db_datareader ADD MEMBER [<func-app-mi-name>];
```

☑️ SQL Server → Networking → enable "Allow Azure services and resources…" (for dev).  
☑️ Expose only views you're OK to query (prefix `vw`), not raw tables with PII.

## Implementation Details

### package.json
```json
{
  "name": "sql-tools-func",
  "version": "1.0.0",
  "type": "module",
  "engines": { "node": ">=20" },
  "dependencies": {
    "@azure/functions": "^4.6.0",
    "@azure/identity": "^4.2.0",
    "mssql": "^10.0.1",
    "dotenv": "^16.4.5"
  },
  "scripts": {
    "start": "func start",
    "test:schema": "curl -s http://localhost:7071/api/sql-schema | jq",
    "test:query": "curl -s -X POST http://localhost:7071/api/sql-query -H 'Content-Type: application/json' -d '{\"sql\":\"SELECT TOP(5) * FROM vwUsers\"}' | jq"
  }
}
```

### host.json
```json
{
  "version": "2.0",
  "extensionBundle": { 
    "id": "Microsoft.Azure.Functions.ExtensionBundle", 
    "version": "[4.*, 5.0.0)" 
  }
}
```

### local.settings.json
```json
{
  "IsEncrypted": false,
  "Values": {
    "AzureWebJobsStorage": "UseDevelopmentStorage=true",
    "FUNCTIONS_WORKER_RUNTIME": "node",
    "SQL_SERVER": "REPLACE.database.windows.net",
    "SQL_DATABASE": "REPLACE",
    "SQL_CONNECTION_STRING": "",
    "SCHEMA_WHITELIST": "vw%",
    "ROW_LIMIT_DEFAULT": "200",
    "TZ": "UTC",
    "NODE_ENV": "development"
  }
}
```

### Core Helpers

#### src/lib/db.js
```javascript
import sql from 'mssql';
import { DefaultAzureCredential } from '@azure/identity';

const server   = process.env.SQL_SERVER;    // <server>.database.windows.net
const database = process.env.SQL_DATABASE;

let poolPromise;

/** Get a pooled AAD-token connection to Azure SQL using Managed Identity. */
export async function getPool() {
  if (poolPromise) return poolPromise;

  // Local dev with connection string (optional)
  if (process.env.SQL_CONNECTION_STRING && process.env.NODE_ENV === 'development') {
    poolPromise = sql.connect(process.env.SQL_CONNECTION_STRING);
    return poolPromise;
  }

  // Production with Managed Identity
  const credential = new DefaultAzureCredential();
  const token = (await credential.getToken('https://database.windows.net/.default')).token;

  const config = {
    server,
    database,
    options: { encrypt: true, enableArithAbort: true },
    authentication: {
      type: 'azure-active-directory-access-token',
      options: { token }
    },
    pool: { max: 5, min: 0, idleTimeoutMillis: 30000 }
  };

  poolPromise = new sql.ConnectionPool(config).connect().catch(err => {
    poolPromise = undefined;
    throw err;
  });
  return poolPromise;
}

/** Runs a parameterized query with a params object like {p1: 123}. */
export async function runQuery(query, params = {}) {
  const pool = await getPool();
  const request = pool.request();
  // Bind parameters (infer basic types)
  for (const [k, v] of Object.entries(params)) {
    if (v === null || v === undefined) continue;
    const t = typeof v;
    if (t === 'number' && Number.isInteger(v)) request.input(k, sql.Int, v);
    else if (t === 'number') request.input(k, sql.Float, v);
    else if (v instanceof Date) request.input(k, sql.DateTime2, v);
    else if (t === 'boolean') request.input(k, sql.Bit, v);
    else request.input(k, sql.NVarChar(sql.MAX), String(v));
  }
  const result = await request.query(query);
  return result;
}
```

#### src/lib/sqlSafety.js
```javascript
/** Validate that the SQL is SELECT-only and safe-ish. Throw on violation. */
export function validateSelectOnly(sql) {
  const s = sql.trim();
  if (!/^select\s/i.test(s)) throw new Error('Only SELECT statements are allowed.');
  // basic keyword & comment blocks ban
  const banned = /\b(delete|insert|update|merge|alter|drop|create|grant|revoke|truncate|exec|execute|xp_|sp_)\b|;|--|\/\*/i;
  if (banned.test(s)) throw new Error('Prohibited keywords or comment markers found.');
  return s;
}

/** Inject TOP(n) after SELECT if not already present. */
export function injectTopLimit(sql, n) {
  const hasTop = /\bselect\s+top\s*\(/i.test(sql);
  if (hasTop) return sql;
  return sql.replace(/^(\s*select\s+)/i, `$1TOP(${n}) `);
}

/** Convert named params style :p1 to @p1 (tedious/mssql named binder). */
export function toTediousNamedParams(sql) {
  return sql.replace(/:([A-Za-z_]\w*)/g, '@$1');
}
```

#### src/lib/schemaHelpers.js
```javascript
/** Get table descriptions for AI context */
export function getTableDescription(tableName) {
  const descriptions = {
    'vwUsers': 'User accounts with registration dates and basic info',
    'vwOrders': 'Customer orders with amounts and statuses',
    'vwProducts': 'Product catalog with pricing and categories',
    // Add your actual table descriptions
  };
  return descriptions[tableName] || '';
}

/** Get column descriptions for better AI understanding */
export function getColumnDescription(tableName, columnName) {
  const descriptions = {
    'vwUsers': {
      'UserId': 'Unique identifier for each user',
      'CreatedAt': 'UTC timestamp when user registered',
      'Country': 'User country code (ISO 2-letter)',
      'IsActive': 'Whether user account is active'
    },
    // Add more tables and columns
  };
  return descriptions[tableName]?.[columnName] || '';
}

/** Get common query templates */
export function getCommonQueryTemplates() {
  return [
    { 
      description: "Count all users", 
      template: "SELECT COUNT(*) as total FROM vwUsers" 
    },
    { 
      description: "Users by date", 
      template: "SELECT COUNT(*) as count, CAST(CreatedAt as DATE) as date FROM vwUsers WHERE CreatedAt >= :start_date GROUP BY CAST(CreatedAt as DATE)" 
    },
    {
      description: "Top N by group",
      template: "SELECT TOP(:limit) GroupColumn, COUNT(*) as count FROM vwTable GROUP BY GroupColumn ORDER BY count DESC"
    }
  ];
}

/** Get sample joins for common relationships */
export function getSampleJoins(tableName, fks) {
  if (!fks || fks.length === 0) return [];
  
  return fks.slice(0, 2).map(fk => ({
    description: `Join ${tableName} with ${fk.ref_table}`,
    template: `SELECT t1.*, t2.* FROM ${tableName} t1 INNER JOIN ${fk.ref_table} t2 ON t1.${fk.column} = t2.${fk.ref_column}`
  }));
}
```

### HTTP Functions

#### src/functions/sqlSchema.js
```javascript
import { app } from '@azure/functions';
import { getPool } from '../lib/db.js';
import { getTableDescription, getColumnDescription, getCommonQueryTemplates, getSampleJoins } from '../lib/schemaHelpers.js';

const WHITELIST = process.env.SCHEMA_WHITELIST || 'vw%';

app.http('sql-schema', {
  methods: ['GET','POST'],
  authLevel: 'function',
  route: 'sql-schema',
  handler: async (req, ctx) => {
    try {
      let filter = null;
      if (req.method === 'POST') {
        const body = await req.json().catch(() => ({}));
        filter = Array.isArray(body?.tables) && body.tables.length ? body.tables : null;
      }

      const pool = await getPool();
      let views;
      if (filter) {
        const list = filter.map((_, i) => `@n${i}`).join(',');
        const r = await pool.request().query(
          `SELECT TABLE_SCHEMA, TABLE_NAME
             FROM INFORMATION_SCHEMA.VIEWS
            WHERE TABLE_NAME IN (${list})`.replace(/@n(\d+)/g, (_, i) => `'${filter[i]}'`)
        );
        views = r.recordset;
      } else {
        const r = await pool.request().query(
          `SELECT TABLE_SCHEMA, TABLE_NAME
             FROM INFORMATION_SCHEMA.VIEWS
            WHERE TABLE_NAME LIKE '${WHITELIST}'`
        );
        views = r.recordset;
      }
      const names = views.map(v => v.TABLE_NAME);
      if (!names.length) {
        return { 
          status: 200, 
          jsonBody: { 
            tables: [], 
            common_queries: getCommonQueryTemplates(),
            generated_at_utc: new Date().toISOString(), 
            notes: `Views matching ${WHITELIST}` 
          } 
        };
      }

      const inList = names.map(n => `'${n}'`).join(',');
      const columns = await pool.request().query(
        `SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE, IS_NULLABLE, CHARACTER_MAXIMUM_LENGTH, ORDINAL_POSITION
           FROM INFORMATION_SCHEMA.COLUMNS
          WHERE TABLE_NAME IN (${inList})
          ORDER BY TABLE_NAME, ORDINAL_POSITION`
      );

      const pkRows = await pool.request().query(
        `SELECT t.name AS table_name, c.name AS column_name
           FROM sys.indexes i
           JOIN sys.index_columns ic ON ic.object_id=i.object_id AND ic.index_id=i.index_id
           JOIN sys.columns c ON c.object_id=ic.object_id AND c.column_id=ic.column_id
           JOIN sys.tables t ON t.object_id=i.object_id
          WHERE i.is_primary_key=1 AND t.name IN (${inList})`
      );

      const fkRows = await pool.request().query(
        `SELECT tp.name AS parent_table, cp.name AS parent_col, tr.name AS ref_table, cr.name AS ref_col
           FROM sys.foreign_keys fk
           JOIN sys.foreign_key_columns fkc ON fkc.constraint_object_id=fk.object_id
           JOIN sys.tables tp ON tp.object_id=fkc.parent_object_id
           JOIN sys.columns cp ON cp.object_id=fkc.parent_object_id AND cp.column_id=fkc.parent_column_id
           JOIN sys.tables tr ON tr.object_id=fkc.referenced_object_id
           JOIN sys.columns cr ON cr.object_id=fkc.referenced_object_id AND cr.column_id=fkc.referenced_column_id
          WHERE tp.name IN (${inList}) AND tr.name IN (${inList})`
      );

      const pkMap = {};
      pkRows.recordset.forEach(r => { (pkMap[r.table_name] ??= new Set()).add(r.column_name); });

      const fkMap = {};
      fkRows.recordset.forEach(r => {
        (fkMap[r.parent_table] ??= []).push({ column: r.parent_col, ref_table: r.ref_table, ref_column: r.ref_col });
      });

      const colMap = {};
      columns.recordset.forEach(r => {
        (colMap[r.TABLE_NAME] ??= []).push({
          name: r.COLUMN_NAME,
          type: r.DATA_TYPE,
          nullable: r.IS_NULLABLE === 'YES',
          ...(r.CHARACTER_MAXIMUM_LENGTH ? { max_len: r.CHARACTER_MAXIMUM_LENGTH } : {})
        });
      });

      const tables = names.map(n => ({
        name: n,
        description: getTableDescription(n),
        columns: (colMap[n] || []).map(c => ({ 
          ...c, 
          pk: pkMap[n]?.has(c.name) || false,
          description: getColumnDescription(n, c.name)
        })),
        fks: fkMap[n] || [],
        sample_joins: getSampleJoins(n, fkMap[n])
      }));

      return { 
        status: 200, 
        jsonBody: { 
          tables, 
          common_queries: getCommonQueryTemplates(),
          generated_at_utc: new Date().toISOString(), 
          notes: `Views matching ${WHITELIST}` 
        } 
      };
    } catch (err) {
      ctx.error(err);
      return { status: 500, jsonBody: { error: err.message || String(err) } };
    }
  }
});
```

#### src/functions/sqlQuery.js
```javascript
import { app } from '@azure/functions';
import { runQuery } from '../lib/db.js';
import { validateSelectOnly, injectTopLimit, toTediousNamedParams } from '../lib/sqlSafety.js';

const ROW_LIMIT_DEFAULT = parseInt(process.env.ROW_LIMIT_DEFAULT || '200', 10);

app.http('sql-query', {
  methods: ['POST'],
  authLevel: 'function',
  route: 'sql-query',
  handler: async (req, ctx) => {
    const startTime = Date.now();
    try {
      const body = await req.json();
      let { sql, params, row_limit } = body || {};
      if (!sql || typeof sql !== 'string') {
        return { status: 400, jsonBody: { error: 'Body.sql (string) is required.' } };
      }
      params = params || {};
      const limit = Math.max(1, Math.min(parseInt(row_limit || ROW_LIMIT_DEFAULT, 10), 5000));

      sql = validateSelectOnly(sql);
      sql = injectTopLimit(sql, limit);
      sql = toTediousNamedParams(sql); // turn :p1 into @p1

      const result = await runQuery(sql, params);
      const columns = result.recordset.columns ? Object.keys(result.recordset.columns) : (result.recordset[0] ? Object.keys(result.recordset[0]) : []);
      const rows = result.recordset.map(r => columns.map(c => r[c]));

      // Log for audit (truncate SQL for security)
      ctx.log('SQL Query Executed', {
        sql: sql.substring(0, 200),
        row_count: rows.length,
        execution_time_ms: Date.now() - startTime,
        user: req.headers['x-ms-client-principal-name'] || 'anonymous',
        source: req.headers['user-agent']?.includes('Connectory') ? 'Copilot Studio via Connectory' : 'Direct'
      });

      return {
        status: 200,
        jsonBody: {
          columns,
          rows,
          row_count: rows.length,
          sql_used: sql,
          execution_time_ms: Date.now() - startTime,
          notes: rows.length >= limit ? `Truncated to TOP(${limit}).` : undefined
        }
      };
    } catch (err) {
      ctx.error(err);
      ctx.log('SQL Query Error', {
        error: err.message,
        execution_time_ms: Date.now() - startTime
      });
      return { status: 500, jsonBody: { error: err.message || String(err) } };
    }
  }
});
```

#### src/app.js
```javascript
// Ensures env is loaded locally; Azure ignores .env and uses App Settings.
import 'dotenv/config';
import './functions/sqlSchema.js';
import './functions/sqlQuery.js';
```

## Deployment Scripts

### scripts/deploy_infra.sh
```bash
#!/usr/bin/env bash
set -euo pipefail

# Usage: ./scripts/deploy_infra.sh <rg> <location> <appname> <storage>
# Example: ./scripts/deploy_infra.sh rg-ai-dev westeurope ai-sql-tools-dev staisqltoolsdev123

RG=${1:?rg}; LOC=${2:?location}; APP=${3:?func app name}; SA=${4:?storage account name}

echo ">> Creating resource group"
az group create -n "$RG" -l "$LOC" >/dev/null

echo ">> Creating storage account (GPv2)"
az storage account create -g "$RG" -n "$SA" -l "$LOC" --sku Standard_LRS --kind StorageV2 >/dev/null

echo ">> Creating Function App (Linux, Consumption, Node 20)"
az functionapp create \
  --resource-group "$RG" \
  --consumption-plan-location "$LOC" \
  --runtime node --runtime-version 20 \
  --functions-version 4 \
  --name "$APP" \
  --storage-account "$SA" >/dev/null

echo ">> Enabling system-assigned managed identity"
az functionapp identity assign -g "$RG" -n "$APP" >/dev/null
MI_PRINCIPAL_ID=$(az functionapp identity show -g "$RG" -n "$APP" --query principalId -o tsv)
echo "   MI principalId: $MI_PRINCIPAL_ID"

echo ">> Setting app settings (edit SQL_* before you run or set after)"
az functionapp config appsettings set -g "$RG" -n "$APP" --settings \
  SCM_DO_BUILD_DURING_DEPLOYMENT=true \
  SQL_SERVER="${SQL_SERVER:-REPLACE.database.windows.net}" \
  SQL_DATABASE="${SQL_DATABASE:-REPLACE}" \
  SCHEMA_WHITELIST="${SCHEMA_WHITELIST:-vw%}" \
  ROW_LIMIT_DEFAULT="${ROW_LIMIT_DEFAULT:-200}" \
  TZ="${TZ:-UTC}" \
  NODE_ENV="production" >/dev/null

cat <<EOF

NEXT STEPS (manual, required):
1) In Azure SQL:
   - Create an AAD user for the Function's Managed Identity and grant read-only:
     CREATE USER [$APP] FROM EXTERNAL PROVIDER;
     ALTER ROLE db_datareader ADD MEMBER [$APP];

2) SQL Server Networking:
   - For dev: enable "Allow Azure services and resources to access this server".
   - For prod: consider Private Endpoint.

3) Deploy code: ./scripts/deploy_func.sh $RG $APP
EOF
```

### scripts/deploy_func.sh
```bash
#!/usr/bin/env bash
set -euo pipefail

# Usage: ./scripts/deploy_func.sh <rg> <appname>
RG=${1:?rg}; APP=${2:?func app name}

echo ">> Installing deps (production)"
npm ci --omit=dev

echo ">> Creating deployment package"
ZIP=package.zip
rm -f "$ZIP"
zip -r "$ZIP" . -x "*.git*" "node_modules/*" ".vscode/*" "package-lock.json" "*.env" >/dev/null

echo ">> Zip deploy (remote build by Oryx)"
az functionapp deployment source config-zip -g "$RG" -n "$APP" --src "$ZIP" >/dev/null

echo ">> Getting function key"
KEY=$(az functionapp function keys list -g "$RG" -n "$APP" --function-name "sql-schema" --query "default" -o tsv 2>/dev/null || echo "<FUNCTION_KEY>")

echo ">> Done. Test endpoints:"
echo "GET  https://$APP.azurewebsites.net/api/sql-schema?code=$KEY"
echo "POST https://$APP.azurewebsites.net/api/sql-query?code=$KEY"
```

## Testing

### Local Testing
```bash
# Terminal 1
npm install
func start

# Terminal 2
curl http://localhost:7071/api/sql-schema | jq

curl -X POST http://localhost:7071/api/sql-query \
  -H "Content-Type: application/json" \
  -d '{"sql":"SELECT TOP(5) UserId, CreatedAt FROM vwUsers ORDER BY CreatedAt DESC"}' | jq

curl -X POST http://localhost:7071/api/sql-query \
  -H "Content-Type: application/json" \
  -d '{"sql":"SELECT COUNT(*) as total FROM vwUsers WHERE CreatedAt >= :start_date", "params":{"start_date":"2024-01-01"}}' | jq
```

## Copilot Studio Integration

### Architecture Flow
```
User Question → Copilot Studio → Connectory → Function App → Azure SQL
                                    ↓
User Answer ← Copilot Studio ← Connectory ← Function Response
```

**Connectory** acts as the middleware layer that:
- Receives requests from Copilot Studio
- Handles authentication and routing to Function App endpoints  
- Transforms responses back to Copilot Studio format
- Manages function keys and endpoint URLs for the Function App

### Connectory Configuration
For Connectory to call the Function App endpoints, configure:
- **Function App URL**: `https://<app-name>.azurewebsites.net`
- **Schema Endpoint**: `/api/sql-schema`
- **Query Endpoint**: `/api/sql-query`  
- **Authentication**: Function key (from deployment script output)

### System Instructions for Copilot Studio
```
When users ask data questions:
1. First call sql_schema (via Connectory) to understand available tables and get query templates
2. Generate SQL using only the tables/columns returned
3. Use parameterized queries with :param syntax for any user-provided values
4. Call sql_query (via Connectory) with the generated SQL
5. Format results in a user-friendly way

Guidelines:
- Always use parameterized queries for safety
- Prefer aggregations over large result sets
- Default to UTC for date calculations unless specified
- Use the common_queries templates when applicable
- Explain what data you're retrieving before showing results
```

### Few-Shot Examples for Copilot
```
User: "How many users signed up yesterday?"
SQL: SELECT COUNT(*) as count FROM vwUsers WHERE CAST(CreatedAt as DATE) = CAST(DATEADD(day, -1, GETUTCDATE()) as DATE)

User: "Show me top 5 countries by users"
SQL: SELECT TOP(5) Country, COUNT(*) as user_count FROM vwUsers GROUP BY Country ORDER BY user_count DESC

User: "Total revenue this month"
SQL: SELECT SUM(Amount) as total_revenue FROM vwOrders WHERE CreatedAt >= DATEADD(month, DATEDIFF(month, 0, GETUTCDATE()), 0)

User: "Users who signed up after January 2024"
SQL: SELECT COUNT(*) as count FROM vwUsers WHERE CreatedAt >= :start_date
Params: {"start_date": "2024-01-01"}
```

## Security Reminders

- ✅ Create the DB reader (AAD user) for the Function App's Managed Identity
- ✅ Whitelist only views you're comfortable exposing (prefix `vw`)
- ✅ No secrets in code; everything from App Settings / MI
- ✅ Log carefully: redact parameter values if they may contain PII  
- ✅ Keep limits: default TOP(200), max 5000; don't stream megabytes to Copilot Studio
- ✅ Function keys managed securely in Connectory configuration
- ✅ Monitor requests: should see user context from Copilot Studio in headers
- ✅ Regular security audits of exposed views and query patterns

## Definition of Done

- [ ] Infra script runs cleanly and prints MI principalId + next steps
- [ ] Code deploy script publishes and endpoints return correct JSON
- [ ] `sql_schema` returns expected views from the sample DB
- [ ] `sql_query` rejects non‑SELECT and executes parameterized SELECTs
- [ ] Query logging works and shows in Application Insights
- [ ] Copilot Studio → Connectory → Function App path answers:
  - "How many users do we have in total?"
  - "How many signed up yesterday?"
  - "Top 5 countries by users"
  - "What's our revenue trend this week?"
- [ ] Error handling returns appropriate messages
- [ ] Performance: queries complete in <2 seconds for typical workloads

=== FILE: ../test_query.json ===
{"sql":"SELECT TOP 5 ProductModelID FROM SalesLT.vProductModelCatalogDescription ORDER BY ProductModelID"}


=== FILE: ../README.md ===
# SQL Tools Function App

Node.js Azure Functions app exposing two endpoints for safe, parameterized read-only SQL access for agents.

## Endpoints
- GET/POST `/api/sql-schema` — returns allowed views, columns, PKs/FKs, sample joins, and common query templates
- POST `/api/sql-query` — validates SELECT-only T-SQL, injects TOP limit, binds params, returns rows

## Local Setup
1. Install Azure Functions Core Tools and Azure CLI.
2. From this folder:
   ```bash
   npm install
   func start
   ```
3. In another terminal, test:
   ```bash
   npm run test:schema
   npm run test:query
   ```

Configure `local.settings.json` values for your SQL if needed. For dev, you can set `SQL_CONNECTION_STRING` and `NODE_ENV=development`.

## Env/App Settings
- `SQL_SERVER` — `<server>.database.windows.net`
- `SQL_DATABASE` — database name
- `SCHEMA_WHITELIST` — pattern for views to expose (e.g., `vw%`)
- `ROW_LIMIT_DEFAULT` — default row cap (e.g., 200)
- `TZ` — `UTC`
- `NODE_ENV` — `development` locally, `production` in Azure

## Deploy Infra (Azure)
```bash
./scripts/deploy_infra.sh <rg> <location> <appname> <storage>
```
Then in Azure SQL (database):
```sql
CREATE USER [<func-app-name>] FROM EXTERNAL PROVIDER;
ALTER ROLE db_datareader ADD MEMBER [<func-app-name>];
```
Enable "Allow Azure services…" for dev, or use Private Endpoints for prod.

## Deploy Code
```bash
./scripts/deploy_func.sh <rg> <appname>
```
The script prints test URLs with function key.

## Notes
- Only SELECT is allowed; banned keywords/comments are filtered.
- TOP(n) is injected if missing; default `ROW_LIMIT_DEFAULT`, max 5000.
- Params use `:name` in SQL and are bound as `@name` to mssql.
- Queries are logged with execution time and row counts.



=== FILE: ../package.json ===
{
  "name": "sql-tools-func",
  "version": "1.0.0",
  "type": "module",
  "main": "src/app.js",
  "engines": { "node": ">=20" },
  "dependencies": {
    "@azure/functions": "^4.6.0",
    "@azure/identity": "^4.2.0",
    "mssql": "^10.0.1",
    "dotenv": "^16.4.5"
  },
  "scripts": {
    "start": "func start",
    "test:schema": "curl -s http://localhost:7071/api/sql-schema | jq",
    "test:query": "curl -s -X POST http://localhost:7071/api/sql-query -H 'Content-Type: application/json' -d '{\"sql\":\"SELECT TOP(5) * FROM vwUsers\"}' | jq"
  }
}



=== FILE: ../.env ===
# Local development environment variables
# Azure ignores this file; use Function App Settings in Azure

# --- App / DB ---
SQL_CONNECTION_STRING="Server=tcp:mysqlserver423e.database.windows.net,1433;Initial Catalog=mysqldb;Persist Security Info=False;User ID=CloudSA08ea5ff0;Password=Password1;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;"
NODE_ENV=development
# Optional for local dev; when set and NODE_ENV=development, connection string is used
SQL_CONNECTION_STRING=
SCHEMA_WHITELIST=v%
ROW_LIMIT_DEFAULT=200
TZ=UTC
NODE_ENV=development

# --- Infra convenience vars for deploy scripts ---
AZ_RG=2025-sql-tools-rg
AZ_LOCATION=uksouth
AZ_FUNC_APP=2025-sql-tools-func
AZ_STORAGE_ACCOUNT=2025sqltoolfuncsstr01


=== FILE: ../scripts/code_aggregator.sh ===
#!/bin/bash

# =================================================================
# Code Aggregator Script - Collect project files into markdown
# =================================================================

# ==================== CONFIGURATION SECTION ====================
# Edit these variables to match your project needs

# Folders to include (space-separated, relative to script location)
# Use repo root by default to include everything relevant
FOLDERS_TO_INCLUDE=(
    ".."
)

# File extensions to include (without the dot)
FILE_EXTENSIONS=(
    "*"
    "dart"
    "js"
    "json"
    "yaml"
    "ts"
    "md"
    "txt"
    "sh"
    "py"
    "html"
    "css"
    "xml"
)

# Files to specifically include (even if extension not in list above)
SPECIFIC_FILES=(
    "../package.json"
    "../host.json"
    "../local.settings.json"
    "../openapi.yaml"
    "../README.md"
    "../test_query.json"
)

# Folders and files to exclude (regex patterns)
EXCLUDE_PATTERNS=(
    "node_modules"
    "\.git"
    "build"
    "\.dart_tool"
    "ios"
    "android"
    "macos" 
    "windows"
    "linux"
    "web"
    "(^|/)test(/|$)"
    "package-lock\.json"
    "pubspec\.lock"
    "\.DS_Store"
    "code-agg-.*\.md"
)

# Output file name (write to repo root)
OUTPUT_FILE="../$(date -u +'%d-%b-%Y')-code-agg-$(date -u +'%H%M').md"

# =================================================================

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Function to check if file should be excluded
should_exclude() {
    local file_path="$1"
    for pattern in "${EXCLUDE_PATTERNS[@]}"; do
        if [[ "$file_path" =~ $pattern ]]; then
            return 0  # Should exclude
        fi
    done
    return 1  # Should not exclude
}

# Function to check if file extension is included
is_extension_included() {
    local file="$1"
    local extension="${file##*.}"
    
    for ext in "${FILE_EXTENSIONS[@]}"; do
        if [[ "$ext" == "*" ]]; then
            return 0  # All extensions are included
        fi
        if [[ "$extension" == "$ext" ]]; then
            return 0  # Extension is included
        fi
    done
    return 1  # Extension not included
}

# Function to check if file is specifically included
is_specific_file() {
    local file="$1"
    local basename=$(basename "$file")
    
    for specific in "${SPECIFIC_FILES[@]}"; do
        if [[ "$basename" == "$specific" ]]; then
            return 0  # File is specifically included
        fi
    done
    return 1  # File not specifically included
}

# Function to get relative path
get_relative_path() {
    local file="$1"
    # Normalize to repo root
    local repo_root="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
    echo "${file#$repo_root/}"
}

# Function to generate tree structure
generate_tree() {
    local temp_file=$(mktemp)
    local files_sorted=$(mktemp)

    # Collect all files that will be included (relative to repo root)
    for folder in "${FOLDERS_TO_INCLUDE[@]}"; do
        if [[ -d "$folder" ]]; then
            while IFS= read -r -d '' file; do
                if should_exclude "$file"; then
                    continue
                fi
                if is_extension_included "$file" || is_specific_file "$file"; then
                    local rel_path=$(get_relative_path "$file")
                    echo "$rel_path" >> "$temp_file"
                fi
            done < <(find "$folder" -type f -print0 2>/dev/null)
        fi
    done

    # Add specific root files (as relative paths)
    for specific_file in "${SPECIFIC_FILES[@]}"; do
        if [[ -f "$specific_file" ]] && ! should_exclude "$specific_file"; then
            local rel_path=$(get_relative_path "$specific_file")
            echo "$rel_path" >> "$temp_file"
        fi
    done

    # Deduplicate and sort files
    sort -u "$temp_file" > "$files_sorted"

    echo "Project Structure"
    echo "=================="

    if command -v tree >/dev/null 2>&1; then
        # Use tree command if available by creating temp dir structure
        local tree_temp=$(mktemp -d)
        # Create files with their directory structure
        while read -r file; do
            local file_dir="$tree_temp/$(dirname "$file")"
            mkdir -p "$file_dir"
            touch "$tree_temp/$file"
        done < "$files_sorted"
        tree "$tree_temp" -I '__pycache__|*.pyc|node_modules' --dirsfirst
        rm -rf "$tree_temp"
    else
        # Fallback: Use ls-style tree with proper indentation
        python3 -c "
import sys
from collections import defaultdict

files = []
with open('$files_sorted', 'r') as f:
    files = [line.strip() for line in f if line.strip()]

# Build directory tree structure
tree = defaultdict(list)
dirs = set()

for file in files:
    parts = file.split('/')
    for i in range(len(parts) - 1):
        parent = '/'.join(parts[:i+1]) if i > 0 else parts[0]
        dirs.add(parent)
        if i == len(parts) - 2:  # This is the parent dir of the file
            tree[parent].append(parts[-1])

# Print tree structure
def print_tree(path, indent=0):
    prefix = '  ' * indent + '├── '
    if path == '.':
        print('.')
        for item in sorted(dirs):
            if '/' not in item:
                print_tree(item, 1)
        # Print root files
        for file in files:
            if '/' not in file:
                print('  ├── ' + file)
    else:
        print(prefix + path.split('/')[-1] + '/')
        # Print subdirectories
        for d in sorted(dirs):
            if d.startswith(path + '/') and d.count('/') == path.count('/') + 1:
                print_tree(d, indent + 1)
        # Print files in this directory
        for file in sorted(tree[path]):
            print('  ' * (indent + 1) + '├── ' + file)

print_tree('.')
" 2>/dev/null || {
            # Ultimate fallback: simple list
            echo "."
            while read -r file; do
                depth=$(echo "$file" | tr -cd '/' | wc -c)
                indent=$(printf "%*s" $((depth * 2)) "")
                basename=$(basename "$file")
                echo "${indent}├── ${basename}"
            done < "$files_sorted"
        }
    fi

    rm -f "$temp_file" "$files_sorted"
}

# Main function to process files
process_files() {
    local total_files=0
    local processed_files=0
    local included_list=$(mktemp)
    
    print_status "Starting code aggregation..."
    
    # Create output file with header
    cat > "$OUTPUT_FILE" << EOF
# Project Code Export
Generated on: $(date)
Folders: ${FOLDERS_TO_INCLUDE[*]}
Extensions: ${FILE_EXTENSIONS[*]}
Specific files: ${SPECIFIC_FILES[*]}

EOF

    # Generate and add tree structure
    generate_tree >> "$OUTPUT_FILE"
    
    echo "" >> "$OUTPUT_FILE"

    # Process each specified folder
    for folder in "${FOLDERS_TO_INCLUDE[@]}"; do
        if [[ -d "$folder" ]]; then
            print_status "Processing folder: $folder"
            
            # Find all files in the folder
            while IFS= read -r -d '' file; do
                ((total_files++))
                
                if should_exclude "$file"; then
                    continue
                fi
                
                if is_extension_included "$file" || is_specific_file "$file"; then
                    local rel_path=$(get_relative_path "$file")
                    if ! grep -Fxq "$rel_path" "$included_list"; then
                        echo "$rel_path" >> "$included_list"
                        ((processed_files++))
                        print_status "Adding: $rel_path"
                        # Add file with simple format
                        echo "=== FILE: $rel_path ===" >> "$OUTPUT_FILE"
                        cat "$file" >> "$OUTPUT_FILE"
                        echo "" >> "$OUTPUT_FILE"
                        echo "" >> "$OUTPUT_FILE"
                    fi
                fi
            done < <(find "$folder" -type f -print0 2>/dev/null)
        else
            print_warning "Folder not found: $folder"
        fi
    done
    
    # Process specific files in root
    print_status "Processing specific root files..."
    for specific_file in "${SPECIFIC_FILES[@]}"; do
        if [[ -f "$specific_file" ]] && ! should_exclude "$specific_file"; then
            local rel_path=$(get_relative_path "$specific_file")
            if ! grep -Fxq "$rel_path" "$included_list"; then
                echo "$rel_path" >> "$included_list"
                ((processed_files++))
                print_status "Adding specific file: $rel_path"
                echo "=== FILE: $rel_path ===" >> "$OUTPUT_FILE"
                cat "$specific_file" >> "$OUTPUT_FILE"
                echo "" >> "$OUTPUT_FILE"
                echo "" >> "$OUTPUT_FILE"
            fi
        fi
    done
    
    # Add summary to end of file
    cat >> "$OUTPUT_FILE" << EOF

=== EXPORT SUMMARY ===
Total files found: $total_files
Files included: $processed_files
Export completed: $(date)
File size: $(du -h "$OUTPUT_FILE" 2>/dev/null | cut -f1 || echo "Unknown")
Generated by Code Aggregator Script
EOF

    print_success "Code aggregation complete!"
    print_success "Files processed: $processed_files/$total_files"
    print_success "Output saved to: $OUTPUT_FILE"
    
    # Show file size and cleanup
    local file_size=$(du -h "$OUTPUT_FILE" | cut -f1)
    print_status "Output file size: $file_size"
    rm -f "$included_list"
}

# Function to show help
show_help() {
    echo "Code Aggregator Script"
    echo "======================"
    echo ""
    echo "Usage: $0 [options]"
    echo ""
    echo "Options:"
    echo "  -h, --help     Show this help message"
    echo "  -c, --config   Show current configuration"
    echo "  -l, --list     List files that would be processed (dry run)"
    echo ""
    echo "Configuration is done by editing the variables at the top of this script."
}

# Function to show current configuration
show_config() {
    echo "Current Configuration:"
    echo "======================"
    echo "Folders to include: ${FOLDERS_TO_INCLUDE[*]}"
    echo "File extensions: ${FILE_EXTENSIONS[*]}"
    echo "Specific files: ${SPECIFIC_FILES[*]}"
    echo "Exclude patterns: ${EXCLUDE_PATTERNS[*]}"
    echo "Output file: $OUTPUT_FILE"
}

# Function to list files (dry run)
list_files() {
    echo "Files that would be processed:"
    echo "=============================="
    
    local count=0
    
    for folder in "${FOLDERS_TO_INCLUDE[@]}"; do
        if [[ -d "$folder" ]]; then
            echo "📁 $folder/"
            while IFS= read -r -d '' file; do
                if should_exclude "$file"; then
                    continue
                fi
                
                if is_extension_included "$file" || is_specific_file "$file"; then
                    ((count++))
                    local rel_path=$(get_relative_path "$file")
                    echo "  📄 $rel_path"
                fi
            done < <(find "$folder" -type f -print0 2>/dev/null)
        fi
    done
    
    echo ""
    echo "Specific root files:"
    for specific_file in "${SPECIFIC_FILES[@]}"; do
        if [[ -f "$specific_file" ]] && ! should_exclude "$specific_file"; then
            ((count++))
            echo "  📄 $specific_file"
        fi
    done
    
    echo ""
    echo "Total files: $count"
}

# Main script logic
case "${1:-}" in
    -h|--help)
        show_help
        ;;
    -c|--config)
        show_config
        ;;
    -l|--list)
        list_files
        ;;
    "")
        process_files
        ;;
    *)
        print_error "Unknown option: $1"
        echo "Use -h or --help for usage information."
        exit 1
        ;;
esac

=== FILE: ../scripts/deploy_infra.sh ===
#!/usr/bin/env bash
set -euo pipefail

# Usage: ./scripts/deploy_infra.sh <rg> <location> <appname> <storage>
# Example: ./scripts/deploy_infra.sh rg-ai-dev westeurope ai-sql-tools-dev staisqltoolsdev123

# Load .env for local convenience (Azure/CI should pass env or args)
if [ -f ".env" ]; then
  set -a
  # shellcheck disable=SC1091
  . ./.env
  set +a
fi

# Resolve inputs: args override env; fallback to env if args omitted
RG="${1:-${AZ_RG:-${RG:-}}}"
LOC="${2:-${AZ_LOCATION:-${LOC:-}}}"
APP="${3:-${AZ_FUNC_APP:-${APP:-}}}"
SA="${4:-${AZ_STORAGE_ACCOUNT:-${SA:-}}}"

if [ -z "${RG:-}" ] || [ -z "${LOC:-}" ] || [ -z "${APP:-}" ] || [ -z "${SA:-}" ]; then
  echo "Usage: ./scripts/deploy_infra.sh <rg> <location> <appname> <storage>" >&2
  echo "Or set AZ_RG, AZ_LOCATION, AZ_FUNC_APP, AZ_STORAGE_ACCOUNT in .env or env." >&2
  exit 1
fi

echo ">> Using RG=$RG LOC=$LOC APP=$APP SA=$SA"

echo ">> Creating resource group"
az group create -n "$RG" -l "$LOC" >/dev/null

echo ">> Creating storage account (GPv2)"
az storage account create -g "$RG" -n "$SA" -l "$LOC" --sku Standard_LRS --kind StorageV2 >/dev/null

echo ">> Creating Function App (Linux, Consumption, Node 20)"
az functionapp create \
  --resource-group "$RG" \
  --consumption-plan-location "$LOC" \
  --runtime node --runtime-version 20 \
  --functions-version 4 \
  --name "$APP" \
  --storage-account "$SA" >/dev/null

echo ">> Enabling system-assigned managed identity"
az functionapp identity assign -g "$RG" -n "$APP" >/dev/null
MI_PRINCIPAL_ID=$(az functionapp identity show -g "$RG" -n "$APP" --query principalId -o tsv)
echo "   MI principalId: $MI_PRINCIPAL_ID"

echo ">> Setting app settings (edit SQL_* before you run or set after)"
az functionapp config appsettings set -g "$RG" -n "$APP" --settings \
  SCM_DO_BUILD_DURING_DEPLOYMENT=true \
  SQL_SERVER="${SQL_SERVER:-REPLACE.database.windows.net}" \
  SQL_DATABASE="${SQL_DATABASE:-REPLACE}" \
  SCHEMA_WHITELIST="${SCHEMA_WHITELIST:-vw%}" \
  ROW_LIMIT_DEFAULT="${ROW_LIMIT_DEFAULT:-200}" \
  TZ="${TZ:-UTC}" \
  NODE_ENV="production" >/dev/null

cat <<EOF

NEXT STEPS (manual, required):
1) In Azure SQL:
   - Create an AAD user for the Function's Managed Identity and grant read-only:
     CREATE USER [$APP] FROM EXTERNAL PROVIDER;
     ALTER ROLE db_datareader ADD MEMBER [$APP];

2) SQL Server Networking:
   - For dev: enable "Allow Azure services and resources to access this server".
   - For prod: consider Private Endpoint.

3) Deploy code: ./scripts/deploy_func.sh $RG $APP
EOF



=== FILE: ../scripts/deploy_func.sh ===
#!/usr/bin/env bash
set -euo pipefail

# Usage: ./scripts/deploy_func.sh <rg> <appname>

# Load .env if present
if [ -f ".env" ]; then
  set -a
  # shellcheck disable=SC1091
  . ./.env
  set +a
fi

# Resolve inputs: args override env; fallback to env
RG="${1:-${AZ_RG:-${RG:-}}}"
APP="${2:-${AZ_FUNC_APP:-${APP:-}}}"

if [ -z "${RG:-}" ] || [ -z "${APP:-}" ]; then
  echo "Usage: ./scripts/deploy_func.sh <rg> <appname>" >&2
  echo "Or set AZ_RG and AZ_FUNC_APP in .env or env." >&2
  exit 1
fi

echo ">> Using RG=$RG APP=$APP"

echo ">> Installing deps (production)"
npm ci --omit=dev

echo ">> Creating deployment package"
ZIP=package.zip
rm -f "$ZIP"
zip -r "$ZIP" . -x "*.git*" "node_modules/*" ".vscode/*" "package-lock.json" "*.env" >/dev/null

echo ">> Zip deploy (remote build by Oryx)"
az functionapp deployment source config-zip -g "$RG" -n "$APP" --src "$ZIP" >/dev/null

echo ">> Getting function key"
KEY=$(az functionapp function keys list -g "$RG" -n "$APP" --function-name "sql-schema" --query "default" -o tsv 2>/dev/null || echo "<FUNCTION_KEY>")

echo ">> Done. Test endpoints:"
echo "GET  https://$APP.azurewebsites.net/api/sql-schema?code=$KEY"
echo "POST https://$APP.azurewebsites.net/api/sql-query?code=$KEY"



=== FILE: ../openapi.yaml ===
swagger: '2.0'
info:
  title: SQL Tools Function API
  version: 1.0.0
  description: |
    Azure Functions API exposing safe, read-only SQL tools for agents.
    - sql-schema: discover allowed views/columns
    - sql-query: execute parameterized SELECT queries with limits and validation

host: 2025-sql-tools-func.azurewebsites.net
basePath: /
schemes:
  - https
consumes:
  - application/json
produces:
  - application/json

security:
  - function_key_header: []

paths:
  /api/sql-schema:
    get:
      summary: Get whitelisted schema (views and columns)
      operationId: getSqlSchema
      responses:
        '200':
          description: Schema information
          schema:
            $ref: '#/definitions/SchemaResponse'
        '500':
          description: Error
          schema:
            $ref: '#/definitions/ErrorResponse'

  /api/sql-query:
    post:
      summary: Execute a safe, parameterized SELECT query
      operationId: postSqlQuery
      parameters:
        - in: body
          name: body
          required: true
          schema:
            $ref: '#/definitions/QueryRequest'
      responses:
        '200':
          description: Query result
          schema:
            $ref: '#/definitions/QueryResponse'
        '400':
          description: Bad request
          schema:
            $ref: '#/definitions/ErrorResponse'
        '500':
          description: Error
          schema:
            $ref: '#/definitions/ErrorResponse'

securityDefinitions:
  function_key_header:
    type: apiKey
    in: header
    name: x-functions-key
    description: Azure Functions function key passed as header

definitions:
  SchemaResponse:
    type: object
    properties:
      tables:
        type: array
        items:
          $ref: '#/definitions/Table'
      common_queries:
        type: array
        items:
          $ref: '#/definitions/CommonQueryTemplate'
      generated_at_utc:
        type: string
        format: date-time
      notes:
        type: string

  Table:
    type: object
    properties:
      name:
        type: string
      description:
        type: string
      columns:
        type: array
        items:
          $ref: '#/definitions/Column'
      fks:
        type: array
        items:
          $ref: '#/definitions/ForeignKey'
      sample_joins:
        type: array
        items:
          $ref: '#/definitions/SampleJoin'

  Column:
    type: object
    properties:
      name:
        type: string
      type:
        type: string
      nullable:
        type: boolean
      max_len:
        type: integer
        x-nullable: true
      pk:
        type: boolean
      description:
        type: string

  ForeignKey:
    type: object
    properties:
      column:
        type: string
      ref_table:
        type: string
      ref_column:
        type: string

  SampleJoin:
    type: object
    properties:
      description:
        type: string
      template:
        type: string

  CommonQueryTemplate:
    type: object
    properties:
      description:
        type: string
      template:
        type: string

  QueryRequest:
    type: object
    required:
      - sql
    properties:
      sql:
        type: string
        description: "SELECT-only T-SQL. MUST include schema prefix (e.g., SalesLT.vGetAllCategories). Named parameters use :name syntax (converted internally to @name)"
      params:
        type: object
        description: Named parameter values for :name placeholders
        additionalProperties: {}
      row_limit:
        type: integer
        minimum: 1
        maximum: 5000
        description: Optional row limit; default from server setting

  QueryResponse:
    type: object
    properties:
      columns:
        type: array
        items:
          type: string
      rows:
        type: array
        description: Rows returned as array of objects keyed by column name.
        items:
          type: object
          additionalProperties: {}
      row_count:
        type: integer
      sql_used:
        type: string
      execution_time_ms:
        type: integer
      notes:
        type: string
        x-nullable: true

  ErrorResponse:
    type: object
    properties:
      error:
        type: string



=== FILE: ../local.settings.json ===
{
  "IsEncrypted": false,
  "Values": {
    "AzureWebJobsStorage": "UseDevelopmentStorage=true",
    "FUNCTIONS_WORKER_RUNTIME": "node",
    "SQL_SERVER": "mysqlserver423e.database.windows.net",
    "SQL_DATABASE": "mysqldb",
    "SQL_CONNECTION_STRING": "Server=tcp:mysqlserver423e.database.windows.net,1433;Initial Catalog=mysqldb;Persist Security Info=False;User ID=CloudSA08ea5ff0;Password=Password1;MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;",
    "SCHEMA_WHITELIST": "v%",
    "ROW_LIMIT_DEFAULT": "200",
    "TZ": "UTC",
    "NODE_ENV": "development"
  }
}



=== FILE: ../src/lib/schemaHelpers.js ===
/** Get table descriptions for AI context */
export function getTableDescription(tableName) {
  const descriptions = {
    'vwUsers': 'User accounts with registration dates and basic info',
    'vwOrders': 'Customer orders with amounts and statuses',
    'vwProducts': 'Product catalog with pricing and categories',
    // Add your actual table descriptions
  };
  return descriptions[tableName] || '';
}

/** Get column descriptions for better AI understanding */
export function getColumnDescription(tableName, columnName) {
  const descriptions = {
    'vwUsers': {
      'UserId': 'Unique identifier for each user',
      'CreatedAt': 'UTC timestamp when user registered',
      'Country': 'User country code (ISO 2-letter)',
      'IsActive': 'Whether user account is active'
    },
    // Add more tables and columns
  };
  return descriptions[tableName]?.[columnName] || '';
}

/** Get common query templates */
export function getCommonQueryTemplates() {
  return [
    { 
      description: "Count all categories", 
      template: "SELECT COUNT(*) as total FROM SalesLT.vGetAllCategories" 
    },
    { 
      description: "Products by category", 
      template: "SELECT ProductCategoryName, COUNT(*) as count FROM SalesLT.vGetAllCategories GROUP BY ProductCategoryName ORDER BY count DESC" 
    },
    {
      description: "Top N by group",
      template: "SELECT TOP(:limit) GroupColumn, COUNT(*) as count FROM SalesLT.vTable GROUP BY GroupColumn ORDER BY count DESC"
    }
  ];
}

/** Get sample joins for common relationships */
export function getSampleJoins(tableName, fks) {
  if (!fks || fks.length === 0) return [];
  
  return fks.slice(0, 2).map(fk => {
    const refTableWithSchema = fk.ref_table.includes('.') ? fk.ref_table : `SalesLT.${fk.ref_table}`;
    return {
      description: `Join ${tableName} with ${refTableWithSchema}`,
      template: `SELECT t1.*, t2.* FROM ${tableName} t1 INNER JOIN ${refTableWithSchema} t2 ON t1.${fk.column} = t2.${fk.ref_column}`
    };
  });
}



=== FILE: ../src/lib/db.js ===
import sql from 'mssql';
import { DefaultAzureCredential } from '@azure/identity';

const server   = process.env.SQL_SERVER;    // <server>.database.windows.net
const database = process.env.SQL_DATABASE;

let poolPromise;

/** Get a pooled AAD-token connection to Azure SQL using Managed Identity. */
export async function getPool() {
  if (poolPromise) return poolPromise;

  // Local dev with connection string (optional)
  if (process.env.SQL_CONNECTION_STRING && process.env.NODE_ENV === 'development') {
    poolPromise = sql.connect(process.env.SQL_CONNECTION_STRING);
    return poolPromise;
  }

  // Production with Managed Identity
  const credential = new DefaultAzureCredential();
  const token = (await credential.getToken('https://database.windows.net/.default')).token;

  const config = {
    server,
    database,
    options: { encrypt: true, enableArithAbort: true },
    authentication: {
      type: 'azure-active-directory-access-token',
      options: { token }
    },
    pool: { max: 5, min: 0, idleTimeoutMillis: 30000 }
  };

  poolPromise = new sql.ConnectionPool(config).connect().catch(err => {
    poolPromise = undefined;
    throw err;
  });
  return poolPromise;
}

/** Runs a parameterized query with a params object like {p1: 123}. */
export async function runQuery(query, params = {}) {
  const pool = await getPool();
  const request = pool.request();
  // Bind parameters (infer basic types)
  for (const [k, v] of Object.entries(params)) {
    if (v === null || v === undefined) continue;
    const t = typeof v;
    if (t === 'number' && Number.isInteger(v)) request.input(k, sql.Int, v);
    else if (t === 'number') request.input(k, sql.Float, v);
    else if (v instanceof Date) request.input(k, sql.DateTime2, v);
    else if (t === 'boolean') request.input(k, sql.Bit, v);
    else request.input(k, sql.NVarChar(sql.MAX), String(v));
  }
  const result = await request.query(query);
  return result;
}



=== FILE: ../src/lib/sqlSafety.js ===
/** Validate that the SQL is SELECT-only and safe-ish. Throw on violation. */
export function validateSelectOnly(sql) {
  const s = sql.trim();
  if (!/^select\s/i.test(s)) throw new Error('Only SELECT statements are allowed.');
  // basic keyword & comment blocks ban
  const banned = /\b(delete|insert|update|merge|alter|drop|create|grant|revoke|truncate|exec|execute|xp_|sp_)\b|;|--|\/\*/i;
  if (banned.test(s)) throw new Error('Prohibited keywords or comment markers found.');
  return s;
}

/** Inject TOP(n) after SELECT if not already present. */
export function injectTopLimit(sql, n) {
  // Check for both TOP(n) and TOP n syntax
  const hasTop = /\bselect\s+top\s*(\(|\d)/i.test(sql);
  if (hasTop) return sql;
  return sql.replace(/^(\s*select\s+)/i, `$1TOP(${n}) `);
}

/** Convert named params style :p1 to @p1 (tedious/mssql named binder). */
export function toTediousNamedParams(sql) {
  return sql.replace(/:([A-Za-z_]\w*)/g, '@$1');
}



=== FILE: ../src/functions/sqlSchema.js ===
import { app } from '@azure/functions';
import { getPool } from '../lib/db.js';
import { getTableDescription, getColumnDescription, getCommonQueryTemplates, getSampleJoins } from '../lib/schemaHelpers.js';

const WHITELIST = process.env.SCHEMA_WHITELIST || 'vw%';

app.http('sql-schema', {
  methods: ['GET','POST'],
  authLevel: 'function',
  route: 'sql-schema',
  handler: async (req, ctx) => {
    try {
      let filter = null;
      if (req.method === 'POST') {
        const body = await req.json().catch(() => ({}));
        filter = Array.isArray(body?.tables) && body.tables.length ? body.tables : null;
      }

      const pool = await getPool();
      let views;
      if (filter) {
        const list = filter.map((_, i) => `@n${i}`).join(',');
        const r = await pool.request().query(
          `SELECT TABLE_SCHEMA, TABLE_NAME
             FROM INFORMATION_SCHEMA.VIEWS
            WHERE TABLE_NAME IN (${list})`.replace(/@n(\d+)/g, (_, i) => `'${filter[i]}'`)
        );
        views = r.recordset;
      } else {
        const r = await pool.request().query(
          `SELECT TABLE_SCHEMA, TABLE_NAME
             FROM INFORMATION_SCHEMA.VIEWS
            WHERE TABLE_NAME LIKE '${WHITELIST}'`
        );
        views = r.recordset;
      }
      const names = views.map(v => v.TABLE_NAME);
      const schemaNames = views.map(v => `${v.TABLE_SCHEMA}.${v.TABLE_NAME}`);
      if (!names.length) {
        return { 
          status: 200, 
          jsonBody: { 
            tables: [], 
            common_queries: getCommonQueryTemplates(),
            generated_at_utc: new Date().toISOString(), 
            notes: `Views matching ${WHITELIST}` 
          } 
        };
      }

      const inList = names.map(n => `'${n}'`).join(',');
      const columns = await pool.request().query(
        `SELECT TABLE_NAME, COLUMN_NAME, DATA_TYPE, IS_NULLABLE, CHARACTER_MAXIMUM_LENGTH, ORDINAL_POSITION
           FROM INFORMATION_SCHEMA.COLUMNS
          WHERE TABLE_NAME IN (${inList})
          ORDER BY TABLE_NAME, ORDINAL_POSITION`
      );

      const pkRows = await pool.request().query(
        `SELECT t.name AS table_name, c.name AS column_name
           FROM sys.indexes i
           JOIN sys.index_columns ic ON ic.object_id=i.object_id AND ic.index_id=i.index_id
           JOIN sys.columns c ON c.object_id=ic.object_id AND c.column_id=ic.column_id
           JOIN sys.tables t ON t.object_id=i.object_id
          WHERE i.is_primary_key=1 AND t.name IN (${inList})`
      );

      const fkRows = await pool.request().query(
        `SELECT tp.name AS parent_table, cp.name AS parent_col, tr.name AS ref_table, cr.name AS ref_col
           FROM sys.foreign_keys fk
           JOIN sys.foreign_key_columns fkc ON fkc.constraint_object_id=fk.object_id
           JOIN sys.tables tp ON tp.object_id=fkc.parent_object_id
           JOIN sys.columns cp ON cp.object_id=fkc.parent_object_id AND cp.column_id=fkc.parent_column_id
           JOIN sys.tables tr ON tr.object_id=fkc.referenced_object_id
           JOIN sys.columns cr ON cr.object_id=fkc.referenced_object_id AND cr.column_id=fkc.referenced_column_id
          WHERE tp.name IN (${inList}) AND tr.name IN (${inList})`
      );

      const pkMap = {};
      pkRows.recordset.forEach(r => { (pkMap[r.table_name] ??= new Set()).add(r.column_name); });

      const fkMap = {};
      fkRows.recordset.forEach(r => {
        (fkMap[r.parent_table] ??= []).push({ column: r.parent_col, ref_table: r.ref_table, ref_column: r.ref_col });
      });

      const colMap = {};
      columns.recordset.forEach(r => {
        (colMap[r.TABLE_NAME] ??= []).push({
          name: r.COLUMN_NAME,
          type: r.DATA_TYPE,
          nullable: r.IS_NULLABLE === 'YES',
          ...(r.CHARACTER_MAXIMUM_LENGTH ? { max_len: r.CHARACTER_MAXIMUM_LENGTH } : {})
        });
      });

      const tables = names.map((n, i) => ({
        name: schemaNames[i],
        description: getTableDescription(n),
        columns: (colMap[n] || []).map(c => ({ 
          ...c, 
          pk: pkMap[n]?.has(c.name) || false,
          description: getColumnDescription(n, c.name)
        })),
        fks: fkMap[n] || [],
        sample_joins: getSampleJoins(schemaNames[i], fkMap[n])
      }));

      return { 
        status: 200, 
        jsonBody: { 
          tables, 
          common_queries: getCommonQueryTemplates(),
          generated_at_utc: new Date().toISOString(), 
          notes: `Views matching ${WHITELIST}` 
        } 
      };
    } catch (err) {
      ctx.error(err);
      return { status: 500, jsonBody: { error: err.message || String(err) } };
    }
  }
});



=== FILE: ../src/functions/sqlQuery.js ===
import { app } from '@azure/functions';
import { runQuery } from '../lib/db.js';
import { validateSelectOnly, injectTopLimit, toTediousNamedParams } from '../lib/sqlSafety.js';

const ROW_LIMIT_DEFAULT = parseInt(process.env.ROW_LIMIT_DEFAULT || '200', 10);

app.http('sql-query', {
  methods: ['POST'],
  authLevel: 'function',
  route: 'sql-query',
  handler: async (req, ctx) => {
    const startTime = Date.now();
    try {
      const body = await req.json();
      let { sql, params, row_limit } = body || {};
      if (!sql || typeof sql !== 'string') {
        return { status: 400, jsonBody: { error: 'Body.sql (string) is required.' } };
      }
      params = params || {};
      const limit = Math.max(1, Math.min(parseInt(row_limit || ROW_LIMIT_DEFAULT, 10), 5000));

      sql = validateSelectOnly(sql);
      sql = injectTopLimit(sql, limit);
      sql = toTediousNamedParams(sql); // turn :p1 into @p1

      const result = await runQuery(sql, params);
      const columns = result.recordset.columns ? Object.keys(result.recordset.columns) : (result.recordset[0] ? Object.keys(result.recordset[0]) : []);
      // Return rows as array of objects keyed by column name for Copilot table coercion
      const rows = result.recordset.map(r => {
        const obj = {};
        for (const c of columns) obj[c] = r[c];
        return obj;
      });

      // Log for audit (truncate SQL for security)
      ctx.log('SQL Query Executed', {
        sql: sql.substring(0, 200),
        row_count: rows.length,
        execution_time_ms: Date.now() - startTime,
        user: req.headers['x-ms-client-principal-name'] || 'anonymous'
      });

      return {
        status: 200,
        jsonBody: {
          columns,
          rows,
          row_count: rows.length,
          sql_used: sql,
          execution_time_ms: Date.now() - startTime,
          notes: rows.length >= limit ? `Truncated to TOP(${limit}).` : undefined
        }
      };
    } catch (err) {
      ctx.error(err);
      ctx.log('SQL Query Error', {
        error: err.message,
        execution_time_ms: Date.now() - startTime
      });
      return { status: 500, jsonBody: { error: err.message || String(err) } };
    }
  }
});



=== FILE: ../src/app.js ===
// Ensures env is loaded locally; Azure ignores .env and uses App Settings.
import 'dotenv/config';
import './functions/sqlSchema.js';
import './functions/sqlQuery.js';




=== EXPORT SUMMARY ===
Total files found: 14809
Files included: 19
Export completed: Wed Aug 27 10:59:47 BST 2025
File size:  64K
Generated by Code Aggregator Script
